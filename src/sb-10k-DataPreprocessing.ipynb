{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAS_PATH = pathlib.Path(\"test/\")\n",
    "CLAS_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas_data = pd.read_csv(CLAS_PATH/\"downloaded.tsv\", sep=\"\\t\", header=None)\n",
    "df_clas_data = df_clas_data[[1,4]]\n",
    "df_clas_data.columns = ['sentiment', 'tweet_text']\n",
    "\n",
    "df_clas_data.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "df_clas_data = df_clas_data[df_clas_data.tweet_text.str.contains(\"Not Available\") == False]\n",
    "\n",
    "mapping = {'positive': 1.0, 'negative': -1.0, 'neutral': 0}\n",
    "df_clas_data = df_clas_data.replace({'sentiment': mapping})\n",
    "\n",
    "#df_clas_data['sentiment'] = df_clas_data['sentiment'].str.strip()\n",
    "df_clas_data['sentiment'] = df_clas_data['sentiment'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Remove punctuation\n",
    "    word = word.strip('\\'\"?!,.():;')\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    # Remove - & '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    return word\n",
    "\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
    "\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    processed_tweet = []\n",
    "    # Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    # Replaces URLs with the word URL\n",
    "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
    "    # Replace @handle with the word USER_MENTION\n",
    "    tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
    "    # Replaces #hashtag with hashtag\n",
    "    tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "    # Remove RT (retweet)\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "    # Replace 2+ dots with space\n",
    "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "    # Strip space, \" and ' from tweet\n",
    "    tweet = tweet.strip(' \"\\'')\n",
    "    # Replace emojis with either EMO_POS or EMO_NEG\n",
    "    tweet = handle_emojis(tweet)\n",
    "    # Replace multiple spaces with a single space\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    words = tweet.split()\n",
    "\n",
    "    for word in words:\n",
    "        word = preprocess_word(word)\n",
    "        if is_valid_word(word):\n",
    "            processed_tweet.append(word)\n",
    "\n",
    "    return ' '.join(processed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas_data['tweet_text'] = df_clas_data['tweet_text'].apply(lambda x: preprocess_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>USER_MENTION so ein hearthstonekey von USER_ME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>tainted talents ateliertagebuch wir sind nicht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>aber wenigstens kommt supernatural heute mal w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>darlehen angebot schufafreie darlehen anbieter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>anruf hardcore teeny vicky carrera hallo mein ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>na wo sind frankens heimliche talente die erst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>breitet sich aus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>USER_MENTION unachtsam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>jobs sales medien tv tele m1 sucht werbe verka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>happy halloween liebe studenten zeigt eure gru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>die app hat mir mb belegten speicher freigemac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ipad leder smart cover schutz etui schale tasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>USER_MENTION moi USER_MENTION USER_MENTION USE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>check out the new page at times about the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>USER_MENTION italienliebhaber vier kleine mit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>USER_MENTION jop und wenn sich die agenda nich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>grunderwerbsteuer wird der immobilienkauf wird...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>gruppenliga sgae mit choteschovsky schneider s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>talente tumblr internet allein sein nerven URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>USER_MENTION ich euch ne gutte nacht my loves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>appleevent viele details sickern vorher durch URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>baden endlich wundervoll URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>USER_MENTION hm was machen wir denn da seine m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>neue nutzungsbedingungen bei kurz vorgestellt URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>USER_MENTION der bereich personal in wiesbaden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>USER_MENTION berichte vertrauensbildende zusic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>schneller limit of listerine fuck down you lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0</td>\n",
       "      <td>affe dachs elefant fledermaus goldfisch hase i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>USER_MENTION USER_MENTION ach ja dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>USER_MENTION honess bekommt ne sehr hohe gesld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9893</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eltern fragen schon warum ich lache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>USER_MENTION decke begraben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>0.0</td>\n",
       "      <td>USER_MENTION ohne wird dieser tag sein volles ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>0.0</td>\n",
       "      <td>erneuerbare energie jobs emobility emobility i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>USER_MENTION koalitionspartner mahnt kraft die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9902</th>\n",
       "      <td>1.0</td>\n",
       "      <td>USER_MENTION duu doch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9903</th>\n",
       "      <td>0.0</td>\n",
       "      <td>unternehmen kunden mit einem goodie locken wen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904</th>\n",
       "      <td>0.0</td>\n",
       "      <td>neues aus der bloggerwelt blogparade blogparad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>0.0</td>\n",
       "      <td>klopp nimmt hummels gegen kritik in schutz URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9906</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>hab morgen nicht viel zeit zu packen noch nich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>0.0</td>\n",
       "      <td>USER_MENTION USER_MENTION it is very common at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>1.0</td>\n",
       "      <td>wir unserem fc carl zeiss viel erfolg beim hei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>0.0</td>\n",
       "      <td>pkw neu kg nicht gebraucht marke m zoll inkl p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>0.0</td>\n",
       "      <td>better dungeons minecraft better dungeons mine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917</th>\n",
       "      <td>1.0</td>\n",
       "      <td>waah the guys are here joining our partyy waah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918</th>\n",
       "      <td>0.0</td>\n",
       "      <td>USER_MENTION erst letztens wurden reaktoren ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9919</th>\n",
       "      <td>0.0</td>\n",
       "      <td>riesiges tunnelsystem die unterwelt des alten ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9922</th>\n",
       "      <td>0.0</td>\n",
       "      <td>angriff auf amazon ebay will waren noch am sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9923</th>\n",
       "      <td>0.0</td>\n",
       "      <td>USER_MENTION bruder kauf das mixtape vol valla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9925</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ja dieser aspekt sollte auch werden URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9927</th>\n",
       "      <td>0.0</td>\n",
       "      <td>URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9928</th>\n",
       "      <td>0.0</td>\n",
       "      <td>nun raucht doch einfach ne friedenspfeife domian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9929</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>USER_MENTION deprimierend king kings queen que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9930</th>\n",
       "      <td>0.0</td>\n",
       "      <td>USER_MENTION network marketing eine reise nach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9931</th>\n",
       "      <td>0.0</td>\n",
       "      <td>wirtschaftsminister mitterlehner zeichnet mit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9932</th>\n",
       "      <td>0.0</td>\n",
       "      <td>clqualifikation zenit mit kantersieg austria z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9934</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>USER_MENTION okay USER_MENTION machse einfach eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9935</th>\n",
       "      <td>0.0</td>\n",
       "      <td>pfeifturmbegehung um uhr altes rathaus ingolst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9936</th>\n",
       "      <td>1.0</td>\n",
       "      <td>ich muss gerade irgendwem der mich nicht judge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9937</th>\n",
       "      <td>1.0</td>\n",
       "      <td>USER_MENTION ich mag ihn auch echt gerne haste...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7130 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                         tweet_text\n",
       "0           1.0  USER_MENTION so ein hearthstonekey von USER_ME...\n",
       "3           0.0  tainted talents ateliertagebuch wir sind nicht...\n",
       "4           0.0  aber wenigstens kommt supernatural heute mal w...\n",
       "6           0.0  darlehen angebot schufafreie darlehen anbieter...\n",
       "7           0.0  anruf hardcore teeny vicky carrera hallo mein ...\n",
       "8           0.0  na wo sind frankens heimliche talente die erst...\n",
       "9           1.0                                   breitet sich aus\n",
       "10         -1.0                             USER_MENTION unachtsam\n",
       "11          0.0  jobs sales medien tv tele m1 sucht werbe verka...\n",
       "13          0.0  happy halloween liebe studenten zeigt eure gru...\n",
       "14          0.0  die app hat mir mb belegten speicher freigemac...\n",
       "18          0.0  ipad leder smart cover schutz etui schale tasc...\n",
       "19         -1.0  USER_MENTION moi USER_MENTION USER_MENTION USE...\n",
       "20          0.0  check out the new page at times about the firs...\n",
       "21          0.0  USER_MENTION italienliebhaber vier kleine mit ...\n",
       "22         -1.0  USER_MENTION jop und wenn sich die agenda nich...\n",
       "23          0.0  grunderwerbsteuer wird der immobilienkauf wird...\n",
       "24          0.0  gruppenliga sgae mit choteschovsky schneider s...\n",
       "25         -1.0     talente tumblr internet allein sein nerven URL\n",
       "26          1.0      USER_MENTION ich euch ne gutte nacht my loves\n",
       "28          0.0  appleevent viele details sickern vorher durch URL\n",
       "30          1.0                       baden endlich wundervoll URL\n",
       "31          0.0  USER_MENTION hm was machen wir denn da seine m...\n",
       "32          0.0  neue nutzungsbedingungen bei kurz vorgestellt URL\n",
       "33          0.0  USER_MENTION der bereich personal in wiesbaden...\n",
       "34          0.0  USER_MENTION berichte vertrauensbildende zusic...\n",
       "36          0.0  schneller limit of listerine fuck down you lik...\n",
       "37          0.0  affe dachs elefant fledermaus goldfisch hase i...\n",
       "38          1.0                USER_MENTION USER_MENTION ach ja dd\n",
       "39         -1.0  USER_MENTION honess bekommt ne sehr hohe gesld...\n",
       "...         ...                                                ...\n",
       "9893        0.0                eltern fragen schon warum ich lache\n",
       "9895       -1.0                        USER_MENTION decke begraben\n",
       "9897        0.0  USER_MENTION ohne wird dieser tag sein volles ...\n",
       "9899        0.0  erneuerbare energie jobs emobility emobility i...\n",
       "9900        0.0  USER_MENTION koalitionspartner mahnt kraft die...\n",
       "9902        1.0                              USER_MENTION duu doch\n",
       "9903        0.0  unternehmen kunden mit einem goodie locken wen...\n",
       "9904        0.0  neues aus der bloggerwelt blogparade blogparad...\n",
       "9905        0.0     klopp nimmt hummels gegen kritik in schutz URL\n",
       "9906       -1.0  hab morgen nicht viel zeit zu packen noch nich...\n",
       "9908        0.0     USER_MENTION USER_MENTION it is very common at\n",
       "9909        1.0  wir unserem fc carl zeiss viel erfolg beim hei...\n",
       "9911        0.0  pkw neu kg nicht gebraucht marke m zoll inkl p...\n",
       "9913        0.0  better dungeons minecraft better dungeons mine...\n",
       "9917        1.0  waah the guys are here joining our partyy waah...\n",
       "9918        0.0  USER_MENTION erst letztens wurden reaktoren ab...\n",
       "9919        0.0  riesiges tunnelsystem die unterwelt des alten ...\n",
       "9922        0.0  angriff auf amazon ebay will waren noch am sel...\n",
       "9923        0.0  USER_MENTION bruder kauf das mixtape vol valla...\n",
       "9925        0.0            ja dieser aspekt sollte auch werden URL\n",
       "9927        0.0                                                URL\n",
       "9928        0.0   nun raucht doch einfach ne friedenspfeife domian\n",
       "9929       -1.0  USER_MENTION deprimierend king kings queen que...\n",
       "9930        0.0  USER_MENTION network marketing eine reise nach...\n",
       "9931        0.0  wirtschaftsminister mitterlehner zeichnet mit ...\n",
       "9932        0.0  clqualifikation zenit mit kantersieg austria z...\n",
       "9934       -1.0   USER_MENTION okay USER_MENTION machse einfach eu\n",
       "9935        0.0  pfeifturmbegehung um uhr altes rathaus ingolst...\n",
       "9936        1.0  ich muss gerade irgendwem der mich nicht judge...\n",
       "9937        1.0  USER_MENTION ich mag ihn auch echt gerne haste...\n",
       "\n",
       "[7130 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clas_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually trim dataset for equal size of classes (for debug purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = df_clas_data.loc[df_clas_data['sentiment'] == -1.0]\n",
    "#a = a.head(1000)\n",
    "#b = df_clas_data.loc[df_clas_data['sentiment'] == 1.0]\n",
    "#b = b.head(1000)\n",
    "#c = df_clas_data.loc[df_clas_data['sentiment'] == 0]\n",
    "#c = c.head(1000)\n",
    "#frames = [a, b, c]\n",
    "#frames = [a, b]\n",
    "#result = pd.concat(frames)\n",
    "#df_clas_data  = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6382, 748)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove neutral comments\n",
    "#df_clas_data = df_clas_data[df_clas_data['sentiment'] != 0].reset_index(drop=True)\n",
    "df_clas_data.to_csv(CLAS_PATH/\"German_Sentiment_Data.csv\", index=False)\n",
    "\n",
    "# Creating train and validation sets\n",
    "np.random.seed(42)\n",
    "trn_keep = np.random.rand(len(df_clas_data))>0.1\n",
    "df_trn = df_clas_data[trn_keep]\n",
    "df_val = df_clas_data[~trn_keep]\n",
    "\n",
    "# Saving train and validation sets to disk\n",
    "df_trn.to_csv(CLAS_PATH/\"German_Sentiment_Data_Train.csv\", header=None, index=False)\n",
    "df_val.to_csv(CLAS_PATH/\"German_Sentiment_Data_Test.csv\", header=None, index=False)\n",
    "\n",
    "len(df_trn),len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10000\n",
    "df_trn = pd.read_csv(CLAS_PATH/\"German_Sentiment_Data_Train.csv\", header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(CLAS_PATH/\"German_Sentiment_Data_Test.csv\", header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    #print(labels)\n",
    "    \n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    \n",
    "    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    #texts = texts.apply(fixup).values.astype(str)\n",
    "\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts)) # splits the list into sublists for processing by each core\n",
    "    # Lower and upper case is inside the tokenizer\n",
    "    return tok, list(labels)\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        #pdb.set_trace()\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "        \n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6382"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
